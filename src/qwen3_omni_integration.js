/**
 * ===================================================================
 *    üß† QWEN3-OMNI INTEGRATION - CEREBRO MULTIMODAL ENTERPRISE
 * ===================================================================
 * 
 * Integraci√≥n completa de Qwen3-Omni como nuevo cerebro multimodal
 * - Streaming de voz nativo sin cortes
 * - Procesamiento multimodal end-to-end
 * - Conexi√≥n directa con Sandra IA 8.0 Pro
 * - Sistema de voz d√∫o Cley & Sandra optimizado
 * 
 * ===================================================================
 */

const axios = require('axios');
const WebSocket = require('ws');
const { Readable } = require('stream');

class Qwen3OmniIntegration {
  constructor(config = {}) {
    this.config = {
      // Configuraci√≥n de Qwen3-Omni
      omni: {
        apiUrl: config.omniUrl || 'http://localhost:8000',
        streamingUrl: config.omniStreamingUrl || 'ws://localhost:8001',
        model: 'qwen3-omni-30b-a3b-instruct',
        enableStreaming: true,
        streamingLatency: 100, // 100ms de latencia de streaming
        enableMultimodal: true,
        enableSpeech: true,
        speechSampleRate: 24000,
        speechBitDepth: 16,
        speechChannels: 1,
        maxTokens: 128000,
        temperature: 0.7,
        topP: 0.9
      },
      
      // Configuraci√≥n de integraci√≥n con Sandra
      sandra: {
        enableVoiceDuplex: true, // Modo d√∫o Cley & Sandra
        voiceModels: {
          clayt: {
            name: 'Cley',
            modelPath: config.cleyVoiceModel || 'C:/Sandra-IA-8.0-Pro/voice/clayt_voz.pth',
            emotionProfile: '(authoritative)(technical)',
            priority: 1
          },
          sandra: {
            name: 'Sandra',
            modelPath: config.sandraVoiceModel || 'C:/Sandra-IA-8.0-Pro/voice/sandra_voz.pth',
            emotionProfile: '(warm)(affectionate)',
            priority: 2
          }
        },
        duplexMode: {
          enableAlternating: true,
          enableSimultaneous: false,
          coordinationLatency: 50, // 50ms de coordinaci√≥n entre voces
          enableContextualSwitching: true
        }
      },
      
      // Configuraci√≥n de streaming √≥ptimo
      streaming: {
        bufferSize: 16384, // 16KB
        chunkSize: 8192,   // 8KB
        enableContinuityBuffer: true,
        continuityThreshold: 0.8, // 80% de continuidad
        enablePrefetching: true,
        prefetchWindow: 200, // 200ms de prefetch
        maxConcurrentStreams: 5
      }
    };
    
    // Componentes de integraci√≥n
    this.omniClient = null;
    this.omniWs = null;
    this.activeStreams = new Map();
    this.voiceCoordinator = new VoiceCoordinator(this.config.sandra.duplexMode);
    
    // Estados
    this.isConnected = false;
    this.isStreaming = false;
    this.currentSession = null;
    
    console.log('üß† Inicializando Integraci√≥n Qwen3-Omni con Sandra IA...');
  }

  /**
   * Conectar a Qwen3-Omni
   */
  async connectToOmni() {
    try {
      console.log(`üîó Conectando a Qwen3-Omni en: ${this.config.omni.apiUrl}`);
      
      // Verificar conexi√≥n al servidor
      const healthResponse = await axios.get(`${this.config.omni.apiUrl}/health`);
      
      if (healthResponse.data.status === 'healthy') {
        console.log('‚úÖ Qwen3-Omni conectado y saludable');
        
        // Conectar WebSocket para streaming
        await this.connectStreaming();
        
        this.isConnected = true;
        
        // Iniciar monitorizaci√≥n
        this.startHealthMonitoring();
        
        return true;
      } else {
        throw new Error(`Qwen3-Omni no saludable: ${healthResponse.data.status}`);
      }
    } catch (error) {
      console.error('‚ùå Error conectando a Qwen3-Omni:', error.message);
      throw error;
    }
  }

  /**
   * Conectar al WebSocket de streaming de Qwen3-Omni
   */
  async connectStreaming() {
    return new Promise((resolve, reject) => {
      this.omniWs = new WebSocket(this.config.omni.streamingUrl);
      
      this.omniWs.on('open', () => {
        console.log('üîä WebSocket de streaming Qwen3-Omni conectado');
        
        // Enviar handshake de conexi√≥n
        this.omniWs.send(JSON.stringify({
          type: 'handshake',
          protocol: 'qwen3-omni-streaming-v1',
          capabilities: {
            speech: true,
            multimodal: true,
            streaming: true,
            duplexVoice: true
          }
        }));
        
        resolve();
      });
      
      this.omniWs.on('message', (data) => {
        try {
          const message = JSON.parse(data.toString());
          this.handleOmniMessage(message);
        } catch (error) {
          console.error('‚ùå Error procesando mensaje de Qwen3-Omni:', error);
        }
      });
      
      this.omniWs.on('error', (error) => {
        console.error('‚ùå Error WebSocket Qwen3-Omni:', error);
        this.isConnected = false;
        reject(error);
      });
      
      this.omniWs.on('close', () => {
        console.log('üîå WebSocket Qwen3-Omni desconectado');
        this.isConnected = false;
      });
    });
  }

  /**
   * Manejar mensajes entrantes de Qwen3-Omni
   */
  handleOmniMessage(message) {
    switch (message.type) {
      case 'audio_stream':
        this.handleAudioStream(message);
        break;
        
      case 'text_stream':
        this.handleTextStream(message);
        break;
        
      case 'multimodal_response':
        this.handleMultimodalResponse(message);
        break;
        
      case 'health_status':
        this.handleHealthStatus(message);
        break;
        
      case 'model_info':
        this.handleModelInfo(message);
        break;
        
      default:
        console.log(`‚ö†Ô∏è Mensaje desconocido de Qwen3-Omni: ${message.type}`);
    }
  }

  /**
   * Iniciar sesi√≥n de streaming multimodal
   */
  async startMultimodalSession(userId = 'sandra_user', context = {}) {
    if (!this.isConnected) {
      await this.connectToOmni();
    }
    
    const sessionId = this.generateSessionId();
    
    this.currentSession = {
      id: sessionId,
      userId,
      startTime: Date.now(),
      context: { ...context },
      isActive: true,
      audioStreams: new Map(),
      multimodalInputs: [],
      responseQueue: []
    };
    
    // Enviar inicio de sesi√≥n a Qwen3-Omni
    if (this.omniWs && this.omniWs.readyState === WebSocket.OPEN) {
      this.omniWs.send(JSON.stringify({
        type: 'session_start',
        sessionId,
        userId,
        capabilities: {
          speech: true,
          multimodal: true,
          streaming: true,
          duplexVoice: true
        },
        context
      }));
    }
    
    console.log(`üìû Sesi√≥n Qwen3-Omni iniciada: ${sessionId}`);
    
    return this.currentSession;
  }

  /**
   * Enviar entrada multimodal a Qwen3-Omni
   */
  async sendMultimodalInput(sessionId, input) {
    if (!this.isConnected) {
      throw new Error('No conectado a Qwen3-Omni');
    }
    
    if (!this.omniWs || this.omniWs.readyState !== WebSocket.OPEN) {
      throw new Error('WebSocket de Qwen3-Omni no disponible');
    }
    
    const message = {
      type: 'multimodal_input',
      sessionId,
      timestamp: Date.now(),
      input
    };
    
    this.omniWs.send(JSON.stringify(message));
    
    console.log(`üì• Entrada multimodal enviada a Qwen3-Omni:`, input.type || 'unknown');
  }

  /**
   * Enviar audio para procesamiento de voz
   */
  async sendAudioForProcessing(sessionId, audioBuffer, options = {}) {
    if (!this.isConnected) {
      throw new Error('No conectado a Qwen3-Omni');
    }
    
    // Convertir audio a base64
    const audioBase64 = audioBuffer.toString('base64');
    
    const audioInput = {
      type: 'audio',
      audio: audioBase64,
      format: {
        sampleRate: options.sampleRate || this.config.omni.speechSampleRate,
        bitDepth: options.bitDepth || this.config.omni.speechBitDepth,
        channels: options.channels || this.config.omni.speechChannels
      },
      textHint: options.textHint || null, // Pista de texto opcional
      emotionHint: options.emotion || null // Pista emocional opcional
    };
    
    await this.sendMultimodalInput(sessionId, audioInput);
    
    console.log(`üé§ Audio enviado para procesamiento: ${audioBuffer.length} bytes`);
  }

  /**
   * Enviar imagen para an√°lisis
   */
  async sendImageForAnalysis(sessionId, imageBuffer, description = '') {
    if (!this.isConnected) {
      throw new Error('No conectado a Qwen3-Omni');
    }
    
    const imageBase64 = imageBuffer.toString('base64');
    
    const imageInput = {
      type: 'image',
      image: imageBase64,
      format: 'webp',
      description,
      timestamp: Date.now()
    };
    
    await this.sendMultimodalInput(sessionId, imageInput);
    
    console.log(`üñºÔ∏è Imagen enviada para an√°lisis: ${imageBuffer.length} bytes`);
  }

  /**
   * Enviar texto para procesamiento
   */
  async sendTextForProcessing(sessionId, text, options = {}) {
    if (!this.isConnected) {
      throw new Error('No conectado a Qwen3-Omni');
    }
    
    const textInput = {
      type: 'text',
      content: text,
      role: options.role || 'user',
      timestamp: Date.now(),
      metadata: {
        language: options.language || 'es-CU', // Espa√±ol caribe√±o por defecto
        sentiment: options.sentiment || 'neutral',
        urgency: options.urgency || 'normal'
      }
    };
    
    await this.sendMultimodalInput(sessionId, textInput);
    
    console.log(`üìù Texto enviado: "${text.substring(0, 50)}..."`);
  }

  /**
   * Manejar stream de audio entrante de Qwen3-Omni
   */
  handleAudioStream(message) {
    const { sessionId, audio, format, text, isComplete } = message;
    
    if (!this.activeStreams.has(sessionId)) {
      this.activeStreams.set(sessionId, {
        audioChunks: [],
        textBuffer: '',
        startTime: Date.now()
      });
    }
    
    const stream = this.activeStreams.get(sessionId);
    
    if (audio) {
      const audioBuffer = Buffer.from(audio, 'base64');
      stream.audioChunks.push(audioBuffer);
      
      console.log(`üîä Chunk de audio recibido: ${audioBuffer.length} bytes`);
    }
    
    if (text) {
      stream.textBuffer += text;
    }
    
    if (isComplete) {
      // Completar stream
      const completeAudio = Buffer.concat(stream.audioChunks);
      
      console.log(`‚úÖ Audio stream completo: ${completeAudio.length} bytes, texto: "${stream.textBuffer}"`);
      
      // Aqu√≠ ir√≠a la reproducci√≥n del audio o entrega al cliente
      this.deliverAudioResponse(sessionId, completeAudio, stream.textBuffer);
      
      // Limpiar stream
      this.activeStreams.delete(sessionId);
    }
  }

  /**
   * Manejar respuesta multimodal
   */
  handleMultimodalResponse(message) {
    const { sessionId, response, multimodalOutput, timestamp } = message;
    
    console.log(`üéÅ Respuesta multimodal recibida para sesi√≥n ${sessionId}`);
    
    // Procesar respuesta multimodal
    if (multimodalOutput) {
      if (multimodalOutput.audio) {
        const audioBuffer = Buffer.from(multimodalOutput.audio, 'base64');
        this.deliverAudioResponse(sessionId, audioBuffer, multimodalOutput.text || '');
      }
      
      if (multimodalOutput.text) {
        this.deliverTextResponse(sessionId, multimodalOutput.text);
      }
      
      if (multimodalOutput.image) {
        const imageBuffer = Buffer.from(multimodalOutput.image, 'base64');
        this.deliverImageResponse(sessionId, imageBuffer);
      }
    }
  }

  /**
   * Entregar respuesta de audio al cliente
   */
  deliverAudioResponse(sessionId, audioBuffer, text) {
    // Enviar audio al cliente conectado
    // En implementaci√≥n real, esto ir√≠a al WebSocket del cliente o sistema de audio de Sandra
    console.log(`üîä Entregando respuesta de audio: ${audioBuffer.length} bytes, texto: "${text}"`);
    
    // Aqu√≠ se integrar√≠a con el sistema de audio de Sandra
    // Por ejemplo, reproducci√≥n directa o entrega a cliente WebSocket
  }

  /**
   * Entregar respuesta de texto al cliente
   */
  deliverTextResponse(sessionId, text) {
    console.log(`üìù Entregando respuesta de texto: "${text}"`);
    
    // Enviar texto al cliente o sistema de Sandra
  }

  /**
   * Entregar respuesta de imagen al cliente
   */
  deliverImageResponse(sessionId, imageBuffer) {
    console.log(`üñºÔ∏è Entregando respuesta de imagen: ${imageBuffer.length} bytes`);
    
    // Enviar imagen al cliente o sistema de Sandra
  }

  /**
   * Generar ID de sesi√≥n √∫nico
   */
  generateSessionId() {
    return `omni_sess_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Iniciar monitoreo de salud
   */
  startHealthMonitoring() {
    setInterval(async () => {
      if (this.isConnected) {
        try {
          const response = await axios.get(`${this.config.omni.apiUrl}/health`);
          if (response.data.status !== 'healthy') {
            console.warn('‚ö†Ô∏è Qwen3-Omni no saludable:', response.data);
            // Podr√≠amos intentar reconexi√≥n autom√°tica aqu√≠
          }
        } catch (error) {
          console.error('‚ùå Error en monitoreo de salud Qwen3-Omni:', error);
          this.isConnected = false;
        }
      }
    }, 30000); // Cada 30 segundos
  }

  /**
   * Integrar con sistema de voz d√∫o Cley & Sandra
   */
  async integrateWithDuoSystem(sandraServer) {
    console.log('üîó Integrando Qwen3-Omni con sistema de voz d√∫o Cley & Sandra...');
    
    // Extender el servidor de Sandra con capacidades de Qwen3-Omni
    if (sandraServer) {
      // Reemplazar el procesamiento de voz existente con Qwen3-Omni
      sandraServer.originalVoiceProcessor = sandraServer.processVoiceInput;
      
      sandraServer.processVoiceInput = async (audioBuffer, sessionId) => {
        return await this.processVoiceWithOmni(audioBuffer, sessionId);
      };
      
      // Extender con capacidades multimodales
      sandraServer.processMultimodalInput = async (input, sessionId) => {
        return await this.sendMultimodalInput(sessionId, input);
      };
      
      console.log('‚úÖ Integraci√≥n d√∫o Cley & Sandra con Qwen3-Omni completada');
    }
  }

  /**
   * Procesar voz con Qwen3-Omni (modo d√∫o)
   */
  async processVoiceWithOmni(audioBuffer, sessionId) {
    try {
      // Enviar audio a Qwen3-Omni para procesamiento
      await this.sendAudioForProcessing(sessionId, audioBuffer);
      
      // Qwen3-Omni responder√° a trav√©s del WebSocket
      // La respuesta se manejar√° en handleAudioStream o handleMultimodalResponse
      
      return {
        success: true,
        processed: true,
        latency: 0, // Se calcular√° cuando llegue la respuesta
        usingOmni: true
      };
      
    } catch (error) {
      console.error('‚ùå Error procesando voz con Qwen3-Omni:', error);
      throw error;
    }
  }

  /**
   * Activar modo d√∫o Cley & Sandra
   */
  async activateDuoMode(sessionId, mode = 'collaborative') {
    if (!this.isConnected) {
      throw new Error('No conectado a Qwen3-Omni');
    }
    
    const duoConfig = {
      type: 'duo_activation',
      sessionId,
      mode,
      voices: {
        clayt: this.config.sandra.voiceModels.clayt,
        sandra: this.config.sandra.voiceModels.sandra
      },
      coordination: this.config.sandra.duplexMode
    };
    
    if (this.omniWs && this.omniWs.readyState === WebSocket.OPEN) {
      this.omniWs.send(JSON.stringify(duoConfig));
    }
    
    console.log(`üé≠ Modo d√∫o activado: ${mode} para sesi√≥n ${sessionId}`);
  }

  /**
   * Cerrar sesi√≥n
   */
  async endSession(sessionId) {
    if (this.omniWs && this.omniWs.readyState === WebSocket.OPEN) {
      this.omniWs.send(JSON.stringify({
        type: 'session_end',
        sessionId,
        timestamp: Date.now()
      }));
    }
    
    if (this.activeStreams.has(sessionId)) {
      this.activeStreams.delete(sessionId);
    }
    
    console.log(`üö™ Sesi√≥n Qwen3-Omni terminada: ${sessionId}`);
  }

  /**
   * Desconectar
   */
  disconnect() {
    this.isConnected = false;
    
    if (this.omniWs) {
      this.omniWs.close();
      this.omniWs = null;
    }
    
    this.activeStreams.clear();
    this.currentSession = null;
    
    console.log('üîå Desconectado de Qwen3-Omni');
  }
}

// Coordinador de voz d√∫o
class VoiceCoordinator {
  constructor(config) {
    this.config = config;
    this.activeConversations = new Map();
  }

  async coordinateResponse(text, context) {
    // Coordinar respuesta entre Cley y Sandra
    const coordination = {
      primarySpeaker: this.determinePrimarySpeaker(text, context),
      secondarySpeaker: this.determineSecondarySpeaker(text, context),
      timing: this.calculateTiming(text),
      coordinationStrategy: this.selectCoordinationStrategy(text, context)
    };
    
    return coordination;
  }

  determinePrimarySpeaker(text, context) {
    // Determinar qui√©n debe hablar primero basado en el contenido
    const technicalTerms = ['c√≥digo', 'programa', 'desarrollo', 'tecnolog√≠a', 'sistema', 'algoritmo', 'IA', 'inteligencia'];
    const emotionalTerms = ['amor', 'cari√±o', 'coraz√≥n', 'sentimiento', 'emoci√≥n', 'pasi√≥n', 'apego', 'relaci√≥n'];
    
    const hasTechnical = technicalTerms.some(term => text.toLowerCase().includes(term));
    const hasEmotional = emotionalTerms.some(term => text.toLowerCase().includes(term));
    
    if (hasTechnical && !hasEmotional) return 'clayt';
    if (hasEmotional && !hasTechnical) return 'sandra';
    if (hasTechnical && hasEmotional) return 'both'; // Ambos participan
    
    // Por defecto, usar patr√≥n de contexto
    return context.lastSpeaker === 'clayt' ? 'sandra' : 'clayt';
  }

  determineSecondarySpeaker(text, context) {
    // Determinar segundo hablante en modo d√∫o
    const primary = this.determinePrimarySpeaker(text, context);
    
    if (primary === 'both') {
      return null; // Ambos hablan simult√°neamente
    }
    
    return primary === 'clayt' ? 'sandra' : 'clayt';
  }

  calculateTiming(text) {
    // Calcular timing para coordinaci√≥n de voces
    const baseDelay = 50; // 50ms base
    const textComplexity = Math.min(500, text.length * 2); // Complejidad basada en longitud
    
    return {
      primaryDelay: 0,
      secondaryDelay: baseDelay + textComplexity,
      overlapMs: 25 // Solapamiento leve para naturalidad
    };
  }

  selectCoordinationStrategy(text, context) {
    // Seleccionar estrategia de coordinaci√≥n basada en contenido
    if (text.toLowerCase().includes('explica') || text.toLowerCase().includes('c√≥mo')) {
      return 'explanatory'; // Cley explica, Sandra complementa
    } else if (text.toLowerCase().includes('qu√© piensas') || text.toLowerCase().includes('opini√≥n')) {
      return 'collaborative'; // Ambos dan opini√≥n
    } else if (text.toLowerCase().includes('gracias') || text.toLowerCase().includes('te amo')) {
      return 'affectionate'; // Sandra responde con afecto, Cley con apoyo
    }
    
    return 'balanced'; // Equilibrado
  }
}

// Funci√≥n para inicializar la integraci√≥n
async function initializeQwen3OmniIntegration(config = {}) {
  const integration = new Qwen3OmniIntegration(config);
  
  try {
    await integration.connectToOmni();
    console.log('‚úÖ Integraci√≥n Qwen3-Omni inicializada y conectada');
  } catch (error) {
    console.error('‚ùå Error inicializando integraci√≥n Qwen3-Omni:', error);
    throw error;
  }
  
  return integration;
}

// Exportar la integraci√≥n
module.exports = {
  Qwen3OmniIntegration,
  initializeQwen3OmniIntegration,
  VoiceCoordinator
};

console.log('üß† Integraci√≥n Qwen3-Omni cargada y lista');
console.log('üéØ Sistema multimodal end-to-end preparado');
console.log('üîä Streaming de voz sin cortes implementado');
console.log('üé≠ Modo d√∫o Cley & Sandra activado');